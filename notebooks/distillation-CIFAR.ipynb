{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# Knowledge Distillation "
      ],
      "metadata": {
        "id": "AtsMO6_Jtl7g"
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "Current state-of-the-art performance in AI and ML is mainly driven by large and complex deep neural network models that often consist of multiple billions of model parameters. Fortunately, for many applications, pre-trained models can be leveraged through transfer learning, avoiding the burden of training large models from scratch.\n",
        "\n",
        "However, as transfer learning from large pre-trained models becomes more prevalent, deploying these large models to run on devices with limited processing power, such as edge devices (i.e. IoT devices), is challenging. While deep learning models often achieve excellent accuracy, they often fail to meet other requirements such as latency and memory footprint.\n",
        "\n",
        "In this notebook, we demonstrate how a compression technique called knowledge distillation (KD) helps to transfer knowledge from a larger into a smaller, more compact neural network model. In this way, we can benefit (partially) from the knowledge of the larger model and still retain the small memory footprint and inference latency of the smaller model."
      ],
      "metadata": {
        "id": "_2-AfH_Gtl7h"
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## What is Knowledge Distillation exactly?"
      ],
      "metadata": {
        "id": "8IoxDKN7tl7i"
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Knowledge-Distillation_1.png?ssl=1\" width=30%></br>\n",
        "<a href=\"https://arxiv.org/pdf/2006.05525.pdf\">Source</a>\n",
        "</center>\n",
        "\n",
        "This notebook focuses on *response-based knowledge distillation*. Response-based knowledge distillation is a compression technique where a student model is optimised to reproduce the outputs of a larger 'teacher' model. The technique is described in the paper by Hinton et al. (2015) ([paper](https://arxiv.org/abs/1503.02531)). \n",
        "\n",
        "The idea behind response based KD is intuitive, we first train a large 'teacher' model and show the predictions made by the teacher to the 'student model'. When training the student model, we calculate the loss based on both the predictions of the student (logits),  and on the logits of the Teacher. Hence, the student learns from both its own predictions as well as the predictions made by the teacher. \n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/0*B8vlOvK1N_CSgZMo\" width=30%></br>\n",
        "<a href=\"https://arxiv.org/pdf/2006.05525.pdf\">Source</a>\n",
        "</center>\n",
        "\n",
        "For more details on other KD approaches see the survey paper by Gou et al. (See [paper](https://arxiv.org/pdf/2006.05525.pdf))."
      ],
      "metadata": {}
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Where is Knowledge Distillation used?\n",
        "\n",
        "A famous example of knowledge distillation is the DistilBERT model. DistilBERT ([link](https://arxiv.org/pdf/1910.01108.pdf)) is a faster and lighter version of the BERT model ([link](https://arxiv.org/abs/1810.04805)). Thanks to knowledge distillation, DistilBERT is 40% smaller, 60% faster, while retaining 97% of the language understanding capabilities."
      ],
      "metadata": {}
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## In this tutorial\n",
        "\n",
        "In this notebook we will use the Dogs vs. Cats dataset from Kaggle.  The dataset contains approx. 25,000 images of cats and dogs. The goal is to train a computer vision model that can predict whether a cat or dog is in an image. The aim is to illustrate how the accuracy of a very simple CNN can be boosted through knowledge distillation from a larger and more complex model (DenseNet 121).\n",
        "\n",
        "- [Setup](#Setup)\n",
        "- [Functions](#Functions)\n",
        "- [Data](#Load-data)\n",
        "- [Experiments](#Experiments)\n",
        "    - Fine tune the teacher model on the Dogs vs. Cats prediction task\n",
        "    - Train the student model without knowledge distillation\n",
        "    - Train the student model with Knowledge distillation\n",
        "- [Conclusion](#Conclusion)"
      ],
      "metadata": {
        "id": "sQ0snsRmtl7i"
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "MSH8xrqitl7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "import numpy as np\n",
        "import PIL\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1686311111988
        },
        "id": "P5Ta6AFetl7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# path to the directory containing the training data\n",
        "data_dir = '../data/train'\n",
        "\n",
        "# Train, validation and test set percentage\n",
        "train_val_test_split = (0.7,0.2,0.1)\n",
        "\n",
        "# Hyperparameters for training our models\n",
        "num_workers=6\n",
        "batch_size = 100\n",
        "epochs = 10\n",
        "lr = 0.001\n",
        "\n",
        "# Knowledge distillation parameters\n",
        "alpha = 0.5\n",
        "temperature = 5"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1686311112084
        },
        "id": "5CuFi559tl7k"
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Functions\n",
        "\n",
        "Here we define a number of python functions and classes that we use in our notebook:\n",
        "\n",
        "- `get_model_size`: to calculate how 'large' a model is in terms of memory footprint. \n",
        "- `DistillationLoss`: a class defining the distillation loss function \n",
        "- `train`: a function to train a pytorch model\n",
        "- `test`: a function to test our trained pytorch models\n",
        "- `train_with_distillation`: similar to the `train` function, except for the fact that the distillation loss is used instead of the regular loss function"
      ],
      "metadata": {
        "id": "yLf0K0Gftl7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_size(model):\n",
        "    \"\"\"function to calculate the model size in MB\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): pytorch model\n",
        "    \"\"\"\n",
        "    param_size = 0\n",
        "    for param in model.parameters():\n",
        "        param_size += param.nelement() * param.element_size()\n",
        "    \n",
        "    buffer_size = 0\n",
        "    for buffer in model.buffers():\n",
        "        buffer_size += buffer.nelement() * buffer.element_size()\n",
        "    \n",
        "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "\n",
        "    return size_all_mb"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1686311112171
        },
        "id": "cizHMWROtl7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DistillationLoss:\n",
        "\n",
        "    \"\"\"Custom loss calculcation combining\n",
        "    the loss of the student model with the distillation loss\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, student_loss, temperature=1, alpha=0.25):\n",
        "        self.student_loss = student_loss\n",
        "        self.distillation_loss = nn.KLDivLoss()\n",
        "        self.temperature = temperature\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def __call__(self, student_logits, student_target_loss, teacher_logits):\n",
        "        distillation_loss = self.distillation_loss(F.log_softmax(student_logits / self.temperature, dim=1),\n",
        "                                                   F.softmax(teacher_logits / self.temperature, dim=1))\n",
        "\n",
        "        loss = (1 - self.alpha) * student_target_loss + (self.alpha * distillation_loss * (self.temperature**2))\n",
        "        return loss, distillation_loss\n"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1686311112261
        },
        "id": "bf9O-R7Ttl7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "\n",
        "    \"\"\"Simple training function looping over a dataloader to optimize a model with given optimizer and loss function.\n",
        "    \"\"\"\n",
        "\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for X, y in tqdm.tqdm(dataloader, desc = \"Training\", unit = \" Iterations\"):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "\n",
        "    \"\"\"test function evaluating a trained model on test data provided through the dataloader argument.\n",
        "    \"\"\"\n",
        "\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in tqdm.tqdm(dataloader, desc = \"Validating\", unit=\"Iterations\"):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "def train_with_distillation(dataloader, student_model, teacher_model, loss_fn, optimizer, alpha=0.25, temperature=1):\n",
        "\n",
        "    \"\"\"training function to train a student_model with knowledge distillation from a teacher_model. \n",
        "    \"\"\"\n",
        "\n",
        "    distillation_loss = DistillationLoss(student_loss=loss_fn, alpha=alpha, temperature=temperature)\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    student_model.train()\n",
        "    teacher_model.eval()\n",
        "\n",
        "    train_loss, train_dist_loss, correct = 0, 0, 0\n",
        "\n",
        "    for X, y in tqdm.tqdm(dataloader, desc = \"Training with Distillation\", unit = \" Iterations\"):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Let student and teacher both make predictions\n",
        "        pred_student = student_model(X)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          pred_teacher = teacher_model(X)\n",
        "\n",
        "        # Compute the regular student loss\n",
        "        student_target_loss = loss_fn(pred_student, y)\n",
        "        # Combine student loss with the loss resulting from difference between student and teacher predictions\n",
        "        loss, dist_loss = distillation_loss(pred_student, student_target_loss, pred_teacher)\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_dist_loss += dist_loss.item()\n",
        "        correct += (pred_student.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #print(\"student loss {}\".format(student_target_loss))\n",
        "        #print(\"distillation loss {}\".format(dist_loss))\n",
        "        #print(\"loss {}\".format(loss))\n",
        "\n",
        "    train_loss /= num_batches\n",
        "    train_dist_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Train Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {train_loss:>8f}, Avg dist loss: {train_dist_loss:>8f}\\n\")\n",
        "        "
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1686311112513
        },
        "id": "gTe4-yoLtl7m"
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Student/Teacher Models\n",
        "\n",
        "In this section we define our Student and Teacher model architectures. For the Teacher we use a pretrained Densenet (densenet 121) with a modified classifier head. For the Student we implement a very simple and shallow CNN network with only three convolutional layers. "
      ],
      "metadata": {
        "id": "Yro6rNOhtl7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Teacher(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = torchvision.models.densenet121(weights='DEFAULT')\n",
        "        for params in self.model.parameters():\n",
        "            params.requires_grad_ = False\n",
        "\n",
        "        num_ftrs = self.model.classifier.in_features\n",
        "        self.model.classifier = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 250),\n",
        "            nn.Linear(250, 50),\n",
        "            nn.Linear(50, 10)\n",
        "            )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1686311112656
        },
        "id": "hcVuMmtLtl7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Student(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # onvolutional layers (3,16,32)\n",
        "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size=(5, 5), stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size=(5, 5), stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size=(3, 3), padding=1)\n",
        "\n",
        "        # conected layers\n",
        "        self.fc1 = nn.Linear(in_features= 64 * 3 * 3, out_features=250)\n",
        "        self.fc2 = nn.Linear(in_features=250, out_features=50)\n",
        "        self.fc3 = nn.Linear(in_features=50, out_features=10)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1686311112752
        },
        "id": "NUtAflDvtl7n"
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# Load Data\n",
        "\n",
        "1. Download the dataset from kaggle: [link](https://www.kaggle.com/c/dogs-vs-cats).\n",
        "2. Unzip the dogs-vs-cats.zip and find the train.zip file. Unzip the train.zip file and put the files in an easy to reach directory. \n",
        "\n",
        "If you are using Google Colab follow the steps below: \n",
        "\n"
      ],
      "metadata": {
        "id": "3pzxfgHdtl7o"
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "**Step 1**: \n",
        "Use below code to upload your kaggle.json to colab environment (you can download kaggle.json from your Profile->Account->API Token)\n",
        "\n",
        "```\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "```\n",
        "\n",
        "**Step 2**:\n",
        "Below code will remove any existing ~/.kaggle directory and create a new one. It will also move your kaggle.json to ~/.kaggle\n",
        "\n",
        "```\n",
        "!rm -r ~/.kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!mv ./kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "```\n",
        "\n",
        "**Step 3**:\n",
        "Download Dataset. \n",
        "\n",
        "```\n",
        "!kaggle competitions download -c dogs-vs-cats\n",
        "```\n",
        "\n",
        "**Step 4**:\n",
        "\n",
        "```\n",
        "!mkdir data\n",
        "!unzip -o -q dogs-vs-cats.zip -d ./data/ \n",
        "!unzip -o -q ./data/train.zip -d ./data/ \n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data transformations for both training and testing phases\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    #transforms.Resize(256),\n",
        "    #transforms.ColorJitter(),\n",
        "    #transforms.RandomCrop(224),\n",
        "    #transforms.RandomHorizontalFlip(),\n",
        "    transforms.Resize((128,128)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((128,128)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1686311112850
        },
        "id": "Ngk9tJKgtl7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='../data', train=True,\n",
        "                                        download=True, transform=train_transform)\n",
        "train_dataloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='../data', train=False,\n",
        "                                       download=True, transform=val_transform)\n",
        "test_dataloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Files already downloaded and verified\nFiles already downloaded and verified\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1686311117326
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# path to the directory containing the training data\n",
        "data_dir = '../data/train'\n"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1686311117464
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# Experiments\n",
        "\n",
        "1. First, we fine-tune the teacher model (DenseNet) on our Dogs vs. Cats prediction task. \n",
        "2. The student model is trained without knowledge distillation\n",
        "3. The student model is trained again but with knowledge distillation leveraging the predictions of the fine-tuned teacher model (see 1.)"
      ],
      "metadata": {
        "id": "r-R3csrdtl7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: {}\".format(device))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Device: cuda\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1686311117603
        },
        "id": "uGcjkBwFtl7q",
        "outputId": "ec052570-5daa-4ef9-8374-f69f1b01b4e1"
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Fine-tune Teacher model on Dogs vs. Cats Prediction Task"
      ],
      "metadata": {
        "id": "fe0VtVlAtl7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_model = Teacher().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(teacher_model.parameters(), lr=lr, amsgrad=True)"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1686311118879
        },
        "id": "UyRflHdVtl7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune the final classification layers of the teacher model\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, teacher_model, loss_fn=criterion, optimizer=optimizer)\n",
        "    test(test_dataloader, teacher_model, loss_fn=criterion)\n",
        "print(\"Done!\")\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1\n-------------------------------\nTest Error: \n Accuracy: 86.6%, Avg loss: 0.390192 \n\nEpoch 2\n-------------------------------\nTest Error: \n Accuracy: 88.9%, Avg loss: 0.336189 \n\nEpoch 3\n-------------------------------\nTest Error: \n Accuracy: 90.9%, Avg loss: 0.283463 \n\nEpoch 4\n-------------------------------\nTest Error: \n Accuracy: 91.5%, Avg loss: 0.267153 \n\nEpoch 5\n-------------------------------\nTest Error: \n Accuracy: 92.0%, Avg loss: 0.264558 \n\nEpoch 6\n-------------------------------\nTest Error: \n Accuracy: 91.0%, Avg loss: 0.292799 \n\nEpoch 7\n-------------------------------\nTest Error: \n Accuracy: 92.2%, Avg loss: 0.256562 \n\nEpoch 8\n-------------------------------\nTest Error: \n Accuracy: 92.6%, Avg loss: 0.269953 \n\nEpoch 9\n-------------------------------\nTest Error: \n Accuracy: 92.7%, Avg loss: 0.273649 \n\nEpoch 10\n-------------------------------\nTest Error: \n Accuracy: 93.7%, Avg loss: 0.238995 \n\nDone!\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Training: 100%|██████████| 500/500 [01:24<00:00,  5.94 Iterations/s]\nValidating: 100%|██████████| 100/100 [00:05<00:00, 19.61Iterations/s]\nTraining: 100%|██████████| 500/500 [01:22<00:00,  6.09 Iterations/s]\nValidating: 100%|██████████| 100/100 [00:05<00:00, 19.76Iterations/s]\nTraining: 100%|██████████| 500/500 [01:22<00:00,  6.05 Iterations/s]\nValidating: 100%|██████████| 100/100 [00:05<00:00, 19.70Iterations/s]\nTraining: 100%|██████████| 500/500 [01:22<00:00,  6.07 Iterations/s]\nValidating: 100%|██████████| 100/100 [00:05<00:00, 19.63Iterations/s]\nTraining: 100%|██████████| 500/500 [01:22<00:00,  6.05 Iterations/s]\nValidating: 100%|██████████| 100/100 [00:05<00:00, 19.64Iterations/s]\nTraining: 100%|██████████| 500/500 [01:22<00:00,  6.04 Iterations/s]\nValidating: 100%|██████████| 100/100 [00:05<00:00, 19.69Iterations/s]\nTraining: 100%|██████████| 500/500 [01:22<00:00,  6.09 Iterations/s]\nValidating: 100%|██████████| 100/100 [00:05<00:00, 19.56Iterations/s]\nTraining: 100%|██████████| 500/500 [01:22<00:00,  6.05 Iterations/s]\nValidating: 100%|██████████| 100/100 [00:05<00:00, 19.61Iterations/s]\nTraining: 100%|██████████| 500/500 [01:22<00:00,  6.07 Iterations/s]\nValidating: 100%|██████████| 100/100 [00:05<00:00, 19.53Iterations/s]\nTraining: 100%|██████████| 500/500 [01:22<00:00,  6.08 Iterations/s]\nValidating: 100%|██████████| 100/100 [00:05<00:00, 19.62Iterations/s]\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1686311996152
        },
        "id": "mGseX0Qotl7s",
        "outputId": "e75cde50-7cc0-4b91-9afb-d889347219b5"
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Train Student Model without Knowledge Distillation"
      ],
      "metadata": {
        "id": "k0H8Lox8tl7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "student_model = Student().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(student_model.parameters(), lr=lr, amsgrad=True)"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1686311996296
        },
        "id": "0CGrbV57tl7s",
        "outputId": "f3b63dff-598a-4dcd-f5ed-f2de48034043"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, student_model, loss_fn=criterion, optimizer=optimizer)\n",
        "    test(test_dataloader, student_model, loss_fn=criterion)\n",
        "print(\"Done!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1\n-------------------------------\nTest Error: \n Accuracy: 45.1%, Avg loss: 1.478924 \n\nEpoch 2\n-------------------------------\nTest Error: \n Accuracy: 50.5%, Avg loss: 1.347503 \n\nEpoch 3\n-------------------------------\nTest Error: \n Accuracy: 57.8%, Avg loss: 1.179652 \n\nEpoch 4\n-------------------------------\nTest Error: \n Accuracy: 56.3%, Avg loss: 1.207788 \n\nEpoch 5\n-------------------------------\nTest Error: \n Accuracy: 60.1%, Avg loss: 1.129663 \n\nEpoch 6\n-------------------------------\nTest Error: \n Accuracy: 64.9%, Avg loss: 0.995375 \n\nEpoch 7\n-------------------------------\nTest Error: \n Accuracy: 66.4%, Avg loss: 0.961396 \n\nEpoch 8\n-------------------------------\nTest Error: \n Accuracy: 66.7%, Avg loss: 0.945566 \n\nEpoch 9\n-------------------------------\nTest Error: \n Accuracy: 68.0%, Avg loss: 0.923723 \n\nEpoch 10\n-------------------------------\nTest Error: \n Accuracy: 68.9%, Avg loss: 0.898444 \n\nDone!\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Training: 100%|██████████| 500/500 [00:14<00:00, 33.70 Iterations/s]\nValidating: 100%|██████████| 100/100 [00:03<00:00, 32.70Iterations/s]\nTraining: 100%|██████████| 500/500 [00:14<00:00, 33.47 Iterations/s]\nValidating: 100%|██████████| 100/100 [00:03<00:00, 31.66Iterations/s]\nTraining: 100%|██████████| 500/500 [00:14<00:00, 33.49 Iterations/s]\nValidating: 100%|██████████| 100/100 [00:03<00:00, 32.53Iterations/s]\nTraining: 100%|██████████| 500/500 [00:14<00:00, 33.60 Iterations/s]\nValidating: 100%|██████████| 100/100 [00:03<00:00, 32.43Iterations/s]\nTraining: 100%|██████████| 500/500 [00:14<00:00, 33.61 Iterations/s]\nValidating: 100%|██████████| 100/100 [00:03<00:00, 32.36Iterations/s]\nTraining: 100%|██████████| 500/500 [00:15<00:00, 33.16 Iterations/s]\nValidating: 100%|██████████| 100/100 [00:03<00:00, 32.04Iterations/s]\nTraining: 100%|██████████| 500/500 [00:14<00:00, 33.59 Iterations/s]\nValidating: 100%|██████████| 100/100 [00:03<00:00, 32.30Iterations/s]\nTraining: 100%|██████████| 500/500 [00:14<00:00, 33.46 Iterations/s]\nValidating: 100%|██████████| 100/100 [00:03<00:00, 32.38Iterations/s]\nTraining: 100%|██████████| 500/500 [00:15<00:00, 33.24 Iterations/s]\nValidating: 100%|██████████| 100/100 [00:03<00:00, 32.04Iterations/s]\nTraining: 100%|██████████| 500/500 [00:14<00:00, 33.42 Iterations/s]\nValidating: 100%|██████████| 100/100 [00:03<00:00, 32.21Iterations/s]\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1686312176672
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Train Student Model with Knowledge Distillation"
      ],
      "metadata": {
        "id": "if6sVwg_tl7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "student_model_distilled = Student()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(student_model_distilled.parameters(), lr=lr, amsgrad=True)\n",
        "teacher_model = teacher_model.to(device)\n",
        "student_model_distilled = student_model_distilled.to(device)"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1686312176798
        },
        "id": "S8aMZvJltl7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_with_distillation(train_dataloader, student_model_distilled, teacher_model, loss_fn=criterion, optimizer=optimizer, alpha=alpha, temperature=temperature)\n",
        "    test(test_dataloader, student_model_distilled, loss_fn=criterion)\n",
        "print(\"Done!\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1\n-------------------------------\nTrain Error: \n Accuracy: 32.7%, Avg loss: 2.334482, Avg dist loss: 0.111679\n\nTest Error: \n Accuracy: 45.0%, Avg loss: 1.557855 \n\nEpoch 2\n-------------------------------\nTrain Error: \n Accuracy: 47.6%, Avg loss: 1.897204, Avg dist loss: 0.089274\n\nTest Error: \n Accuracy: 50.6%, Avg loss: 1.471544 \n\nEpoch 3\n-------------------------------\nTrain Error: \n Accuracy: 53.1%, Avg loss: 1.720639, Avg dist loss: 0.080178\n\nTest Error: \n Accuracy: 55.7%, Avg loss: 1.389279 \n\nEpoch 4\n-------------------------------\nTrain Error: \n Accuracy: 57.7%, Avg loss: 1.571372, Avg dist loss: 0.072409\n\nTest Error: \n Accuracy: 57.5%, Avg loss: 1.355817 \n\nEpoch 5\n-------------------------------\nTrain Error: \n Accuracy: 61.6%, Avg loss: 1.440392, Avg dist loss: 0.065861\n\nTest Error: \n Accuracy: 60.4%, Avg loss: 1.291276 \n\nEpoch 6\n-------------------------------\nTrain Error: \n Accuracy: 64.4%, Avg loss: 1.337352, Avg dist loss: 0.060624\n\nTest Error: \n Accuracy: 62.5%, Avg loss: 1.268813 \n\nEpoch 7\n-------------------------------\nTrain Error: \n Accuracy: 67.4%, Avg loss: 1.234980, Avg dist loss: 0.055598\n\nTest Error: \n Accuracy: 65.6%, Avg loss: 1.145284 \n\nEpoch 8\n-------------------------------\nTrain Error: \n Accuracy: 69.5%, Avg loss: 1.152259, Avg dist loss: 0.051877\n\nTest Error: \n Accuracy: 67.5%, Avg loss: 1.201062 \n\nEpoch 9\n-------------------------------\nTrain Error: \n Accuracy: 71.3%, Avg loss: 1.078688, Avg dist loss: 0.048376\n\nTest Error: \n Accuracy: 68.0%, Avg loss: 1.134984 \n\nEpoch 10\n-------------------------------\nTrain Error: \n Accuracy: 72.9%, Avg loss: 1.019069, Avg dist loss: 0.045638\n\nTest Error: \n Accuracy: 68.8%, Avg loss: 1.170198 \n\nDone!\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Training with Distillation:   0%|          | 0/500 [00:00<?, ? Iterations/s]/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/torch/nn/functional.py:2904: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n  warnings.warn(\nTraining with Distillation: 100%|██████████| 500/500 [00:27<00:00, 18.23 Iterations/s]\nValidating: 100%|██████████| 100/100 [00:03<00:00, 32.26Iterations/s]\nTraining with Distillation: 100%|██████████| 500/500 [00:27<00:00, 18.24 Iterations/s]\nValidating: 100%|██████████| 100/100 [00:03<00:00, 32.30Iterations/s]\nTraining with Distillation: 100%|██████████| 500/500 [00:27<00:00, 18.31 Iterations/s]\nValidating: 100%|██████████| 100/100 [00:03<00:00, 32.14Iterations/s]\nTraining with Distillation: 100%|██████████| 500/500 [00:27<00:00, 18.29 Iterations/s]\nValidating: 100%|██████████| 100/100 [00:03<00:00, 32.10Iterations/s]\nTraining with Distillation: 100%|██████████| 500/500 [00:27<00:00, 18.22 Iterations/s]\nValidating: 100%|██████████| 100/100 [00:03<00:00, 32.33Iterations/s]\nTraining with Distillation: 100%|██████████| 500/500 [00:27<00:00, 18.26 Iterations/s]\nValidating: 100%|██████████| 100/100 [00:03<00:00, 32.16Iterations/s]\nTraining with Distillation: 100%|██████████| 500/500 [00:27<00:00, 18.27 Iterations/s]\nValidating: 100%|██████████| 100/100 [00:03<00:00, 32.12Iterations/s]\nTraining with Distillation: 100%|██████████| 500/500 [00:27<00:00, 18.29 Iterations/s]\nValidating: 100%|██████████| 100/100 [00:03<00:00, 31.51Iterations/s]\nTraining with Distillation: 100%|██████████| 500/500 [00:27<00:00, 18.28 Iterations/s]\nValidating: 100%|██████████| 100/100 [00:03<00:00, 32.54Iterations/s]\nTraining with Distillation: 100%|██████████| 500/500 [00:27<00:00, 18.24 Iterations/s]\nValidating: 100%|██████████| 100/100 [00:03<00:00, 32.31Iterations/s]\n"
        }
      ],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1686312481677
        },
        "id": "VyAFP1bhtl7t"
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we compare the performance of our two student models, one with knowledge distillation from a teacher model and a second one without. \n",
        "\n",
        "The accuracy is measured on part of our dataset (test set) that none of the models has seen during training. \n",
        "\n",
        "How good are the students in telling cats and dogs apart?"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Teacher model \n",
        "test(test_dataloader, teacher_model, criterion)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Validating: 100%|██████████| 100/100 [00:05<00:00, 19.49Iterations/s]\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Test Error: \n Accuracy: 93.7%, Avg loss: 0.238995 \n\n"
        }
      ],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1686312486831
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Student model with knowledge distillation.\n",
        "test(test_dataloader, student_model_distilled, criterion)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Validating: 100%|██████████| 100/100 [00:03<00:00, 32.28Iterations/s]\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Test Error: \n Accuracy: 68.8%, Avg loss: 1.170198 \n\n"
        }
      ],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1686312489945
        },
        "id": "1ZlGM5Fjtl7u",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputId": "532c4009-60a1-4dd1-adc4-675fe0ff6062"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Student model without knowledge distillation.\n",
        "test(test_dataloader, student_model, criterion)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Validating: 100%|██████████| 100/100 [00:03<00:00, 31.98Iterations/s]\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Test Error: \n Accuracy: 68.9%, Avg loss: 0.898444 \n\n"
        }
      ],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1686312493072
        },
        "id": "h2gYI9fRtl7u",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputId": "38bd4f3a-45c8-4594-873d-e2294ce364e8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result can vary between individual runs due to the stochastic nature of these models. However, the knowledge distillation should result in an increase in model accuracy in the range of 3-6 percentage points. \n",
        "\n",
        "Besides accuracy, model size also plays an important role. The aim of this experiment was to show how smaller models can learn from larger, more complex models through KD. Let's inspect how much smaller our student models are in comparison to the teacher model:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the model size \n",
        "model_size_student = get_model_size(student_model)\n",
        "model_size_teacher = get_model_size(teacher_model)\n",
        "\n",
        "print('model size teachermodel : {:.3f}MB'.format(model_size_teacher))\n",
        "print('model size student model: {:.3f}MB'.format(model_size_student))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "model size teachermodel : 27.874MB\nmodel size student model: 0.724MB\n"
        }
      ],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1686312493228
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "percentage_reduction = ((model_size_student - model_size_teacher)/model_size_teacher)*100\n",
        "print('reduction in model size: {:.2f}%'.format(percentage_reduction))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "reduction in model size: -97.40%\n"
        }
      ],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1686312493333
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# References \n",
        "- [Maximizing Model Performance with Knowlegde Distillation](https://medium.com/artificialis/maximizing-model-performance-with-knowledge-distillation-in-pytorch-12b3960a486a)\n",
        "- [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)\n",
        "- [Knowledge Distillation a Survey](https://arxiv.org/abs/2006.05525)\n",
        "\n",
        "\n",
        "## Dataroots blog posts on model compression\n",
        "- https://dataroots.io/research/contributions/deep-learning-model-compression/?ref=dataroots.ghost.io\n",
        "- https://dataroots.io/research/contributions/model_compression/"
      ],
      "metadata": {
        "id": "g0kmZs-qtl7u",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "orig_nbformat": 4,
    "accelerator": "GPU",
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.8 - Pytorch and Tensorflow"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}