{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "liHnKN3Dtl7e",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "https://arxiv.org/pdf/1503.02531.pdf"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AtsMO6_Jtl7g"
      },
      "source": [
        "# Knowledge Distillation "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_2-AfH_Gtl7h"
      },
      "source": [
        "Current state-of-the-art performance in AI and ML is mainly driven by large and complex deep neural network models that often consist of multiple billions of model parameters. Fortunately, for many applications, pre-trained models can be leveraged through transfer learning, avoiding the burden of training large models from scratch.\n",
        "\n",
        "However, as transfer learning from large pre-trained models becomes more prevalent, deploying these large models to run on devices with limited processing power, such as edge devices (i.e. IoT devices), is challenging. While deep learning models often achieve excellent accuracy, they often fail to meet other requirements such as latency and memory footprint.\n",
        "\n",
        "In this notebook, we demonstrate how a compression technique called knowledge distillation (KD) helps to transfer knowledge from a larger into a smaller, more compact neural network model. In this way, we can benefit (partially) from the knowledge of the larger model and still retain the small memory footprint and inference latency of the smaller model."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8IoxDKN7tl7i"
      },
      "source": [
        "## What is Knowledge Distillation exactly?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<center>\n",
        "<img src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Knowledge-Distillation_1.png?ssl=1\" width=30%></br>\n",
        "<a href=\"https://arxiv.org/pdf/2006.05525.pdf\">Source</a>\n",
        "</center>\n",
        "\n",
        "This notebook focuses on *response-based knowledge distillation*. Response-based knowledge distillation is a compression technique where a student model is optimised to reproduce the outputs of a larger 'teacher' model. The technique is described in the paper by Hinton et al. (2015) ([paper](https://arxiv.org/abs/1503.02531)). \n",
        "\n",
        "The idea behind response based KD is intuitive, we first train a large 'teacher' model and show the predictions made by the teacher to the 'student model'. When training the student model, we calculate the loss based on both the predictions of the student (logits),  and on the logits of the Teacher. Hence, the student learns from both its own predictions as well as the predictions made by the teacher. \n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/0*B8vlOvK1N_CSgZMo\" width=30%></br>\n",
        "<a href=\"https://arxiv.org/pdf/2006.05525.pdf\">Source</a>\n",
        "</center>\n",
        "\n",
        "For more details on other KD approaches see the survey paper by Gou et al. (See [paper](https://arxiv.org/pdf/2006.05525.pdf))."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Where is Knowledge Distillation used?\n",
        "\n",
        "A famous example of knowledge distillation is the DistilBERT model. DistilBERT ([link](https://arxiv.org/pdf/1910.01108.pdf)) is a faster and lighter version of the BERT model ([link](https://arxiv.org/abs/1810.04805)). Thanks to knowledge distillation, DistilBERT is 40% smaller, 60% faster, while retaining 97% of the language understanding capabilities."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ0snsRmtl7i"
      },
      "source": [
        "## In this tutorial\n",
        "\n",
        "In this notebook we will use the Dogs vs. Cats dataset from Kaggle.  The dataset contains approx. 25,000 images of cats and dogs. The goal is to train a computer vision model that can predict whether a cat or dog is in an image. The aim is to illustrate how the accuracy of a very simple CNN can be boosted through knowledge distillation from a larger and more complex model (DenseNet 121).\n",
        "\n",
        "- [Setup](#Setup)\n",
        "- [Functions](#Functions)\n",
        "- [Data](#Load-data)\n",
        "- [Experiments](#Experiments)\n",
        "    - Fine tune the teacher model on the Dogs vs. Cats prediction task\n",
        "    - Train the student model without knowledge distillation\n",
        "    - Train the student model with Knowledge distillation\n",
        "- [Conclusion](#Conclusion)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MSH8xrqitl7i"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1686129162790
        },
        "id": "P5Ta6AFetl7k"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import tqdm\n",
        "import numpy as np\n",
        "import PIL\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1686129163516
        },
        "id": "5CuFi559tl7k"
      },
      "outputs": [],
      "source": [
        "# path to the directory containing the training data\n",
        "data_dir = '../data/train'\n",
        "\n",
        "# Train, validation and test set percentage\n",
        "train_val_test_split = (0.7,0.2,0.1)\n",
        "\n",
        "# Hyperparameters for training our models\n",
        "num_workers=6\n",
        "batch_size = 100\n",
        "epochs = 10\n",
        "lr = 0.001"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yLf0K0Gftl7l"
      },
      "source": [
        "## Functions\n",
        "\n",
        "Here we define a number of python functions and classes that we use in our notebook:\n",
        "\n",
        "- `get_model_size`: to calculate how 'large' a model is in terms of memory footprint. \n",
        "- `DistillationLoss`: a class defining the distillation loss function \n",
        "- `train`: a function to train a pytorch model\n",
        "- `test`: a function to test our trained pytorch models\n",
        "- `train_with_distillation`: similar to the `train` function, except for the fact that the distillation loss is used instead of the regular loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1686129163776
        },
        "id": "cizHMWROtl7l"
      },
      "outputs": [],
      "source": [
        "def get_model_size(model):\n",
        "    \"\"\"function to calculate the model size in MB\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): pytorch model\n",
        "    \"\"\"\n",
        "    param_size = 0\n",
        "    for param in model.parameters():\n",
        "        param_size += param.nelement() * param.element_size()\n",
        "    \n",
        "    buffer_size = 0\n",
        "    for buffer in model.buffers():\n",
        "        buffer_size += buffer.nelement() * buffer.element_size()\n",
        "    \n",
        "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "\n",
        "    return size_all_mb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1686129164457
        },
        "id": "bf9O-R7Ttl7m"
      },
      "outputs": [],
      "source": [
        "class DistillationLoss:\n",
        "\n",
        "    \"\"\"Custom loss calculcation combining\n",
        "    the loss of the student model with the distillation loss\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, student_loss, temperature=1, alpha=0.25):\n",
        "        self.student_loss = student_loss\n",
        "        self.distillation_loss = nn.KLDivLoss()\n",
        "        self.temperature = 1\n",
        "        self.alpha = 0.25\n",
        "\n",
        "    def __call__(self, student_logits, student_target_loss, teacher_logits):\n",
        "        distillation_loss = self.distillation_loss(\n",
        "            F.log_softmax(student_logits / self.temperature, dim=1),\n",
        "            F.softmax(teacher_logits/self.temperature, dim=1))\n",
        "        loss = (1 - self.alpha) * student_target_loss \\\n",
        "            + self.alpha * distillation_loss\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1686129164584
        },
        "id": "gTe4-yoLtl7m"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "\n",
        "    \"\"\"Simple training function looping over a dataloader to optimize a model with given optimizer and loss function.\n",
        "    \"\"\"\n",
        "\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for X, y in tqdm.tqdm(dataloader, desc = \"Training\", unit = \" Iterations\"):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "\n",
        "    \"\"\"test function evaluating a trained model on test data provided through the dataloader argument.\n",
        "    \"\"\"\n",
        "\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in tqdm.tqdm(dataloader, desc = \"Validating\", unit=\"Iterations\"):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "def train_with_distillation(dataloader, student_model, teacher_model, loss_fn, optimizer):\n",
        "\n",
        "    \"\"\"training function to train a student_model with knowledge distillation from a teacher_model. \n",
        "    \"\"\"\n",
        "\n",
        "    distillation_loss = DistillationLoss(student_loss=loss_fn)\n",
        "    size = len(dataloader.dataset)\n",
        "    student_model.train()\n",
        "    teacher_model.eval()\n",
        "\n",
        "    for X, y in tqdm.tqdm(dataloader, desc = \"Training with Distillation\", unit = \" Iterations\"):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Let student and teacher both make predictions\n",
        "        pred_student = student_model(X)\n",
        "        pred_teacher = teacher_model(X)\n",
        "\n",
        "        # Compute the regular student loss\n",
        "        student_target_loss = loss_fn(pred_student, y)\n",
        "        # Combine student loss with the loss resulting from difference between student and teacher predictions\n",
        "        loss = distillation_loss(pred_student, student_target_loss, pred_teacher)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Yro6rNOhtl7n"
      },
      "source": [
        "## Student/Teacher Models\n",
        "\n",
        "In this section we define our Student and Teacher model architectures. For the Teacher we use a pretrained Densenet (densenet 121) with a modified classifier head. For the Student we implement a very simple and shallow CNN network with only three convolutional layers. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1686129164726
        },
        "id": "hcVuMmtLtl7n"
      },
      "outputs": [],
      "source": [
        "class Teacher(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = torchvision.models.densenet121(weights='DEFAULT')\n",
        "        for params in self.model.parameters():\n",
        "            params.requires_grad_ = False\n",
        "\n",
        "        num_ftrs = self.model.classifier.in_features\n",
        "        self.model.classifier = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 500),\n",
        "            nn.Linear(500, 2)\n",
        "            )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1686129165406
        },
        "id": "NUtAflDvtl7n"
      },
      "outputs": [],
      "source": [
        "class Student(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # onvolutional layers (3,16,32)\n",
        "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size=(5, 5), stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size=(5, 5), stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size=(3, 3), padding=1)\n",
        "\n",
        "        # conected layers\n",
        "        self.fc1 = nn.Linear(in_features= 64 * 3 * 3, out_features=500)\n",
        "        self.fc2 = nn.Linear(in_features=500, out_features=50)\n",
        "        self.fc3 = nn.Linear(in_features=50, out_features=2)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3pzxfgHdtl7o"
      },
      "source": [
        "# Load Data\n",
        "\n",
        "1. Download the dataset from kaggle: [link](https://www.kaggle.com/c/dogs-vs-cats).\n",
        "2. Unzip the dogs-vs-cats.zip and find the train.zip file. Unzip the train.zip file and put the files in an easy to reach directory. \n",
        "\n",
        "If you are using Google Colab follow these steps: \n",
        "1. Upload your Kaggle API key (Json file)\n",
        "2. Move the API key in the right location\n",
        "3. \n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 1**: \n",
        "Use below code to upload your kaggle.json to colab environment (you can download kaggle.json from your Profile->Account->API Token)\n",
        "\n",
        "```\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "```\n",
        "\n",
        "**Step 2**:\n",
        "Below code will remove any existing ~/.kaggle directory and create a new one. It will also move your kaggle.json to ~/.kaggle\n",
        "\n",
        "```\n",
        "!rm -r ~/.kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!mv ./kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "```\n",
        "\n",
        "**Step 3**:\n",
        "Download Dataset. \n",
        "\n",
        "```\n",
        "!kaggle competitions download -c dogs-vs-cats\n",
        "```\n",
        "\n",
        "**Step 4**:\n",
        "\n",
        "```\n",
        "!mkdir data\n",
        "!unzip -o -q dogs-vs-cats.zip -d ./data/ \n",
        "!unzip -o -q ./data/train.zip -d ./data/ \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# path to the directory containing the training data\n",
        "data_dir = '../data/train'\n",
        "\n",
        "# Select only the image files from your data directory\n",
        "files = os.listdir(data_dir)\n",
        "files = [f for f in files if '.jpg' in f]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1686129168482
        },
        "id": "Ngk9tJKgtl7p"
      },
      "outputs": [],
      "source": [
        "# Define data transformations for both training and testing phases\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.ColorJitter(),\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.Resize((128,128)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((128,128)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1686129168589
        },
        "id": "32kWesurxanW",
        "outputId": "3a4becac-fed9-484b-e9fc-73f73fbaf6af"
      },
      "outputs": [],
      "source": [
        "class DogsVsCatsDataset(Dataset):\n",
        "    def __init__(self, file_list, dir, mode='train', transform = val_transform):\n",
        "        self.file_list = file_list\n",
        "        self.dir = dir\n",
        "        #self.mode= mode\n",
        "        self.transform = transform\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img = PIL.Image.open(os.path.join(self.dir, self.file_list[idx]))\n",
        "        img = self.transform(img)\n",
        "        img = np.array(img)\n",
        "        if 'dog' in self.file_list[idx]:\n",
        "            self.label = 1\n",
        "        else:\n",
        "            self.label = 0\n",
        "        return img.astype('float32'), self.label\n",
        "\n",
        "\n",
        "train_files, test_files = train_test_split(files, \n",
        "                                    test_size=train_val_test_split[2], \n",
        "                                    random_state=42\n",
        "                                    )\n",
        "train_files, val_files = train_test_split(train_files,\n",
        "                                    test_size=train_val_test_split[1]/train_val_test_split[0], \n",
        "                                    random_state=42\n",
        "                                    )\n",
        "\n",
        "train_dataset = DogsVsCatsDataset(train_files, dir = data_dir, transform = train_transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "val_dataset = DogsVsCatsDataset(val_files, dir = data_dir, transform = val_transform)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size = batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "test_dataset = DogsVsCatsDataset(test_files, dir = data_dir, transform = val_transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle=False, num_workers=num_workers)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "r-R3csrdtl7q"
      },
      "source": [
        "# Experiments\n",
        "\n",
        "1. First, we fine-tune the teacher model (DenseNet) on our Dogs vs. Cats prediction task. \n",
        "2. The student model is trained without knowledge distillation\n",
        "3. The student model is trained again but now with knowledge distillation leveraging the predictions of the fine-tuned teacher model (see 1.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1686129168859
        },
        "id": "uGcjkBwFtl7q",
        "outputId": "ec052570-5daa-4ef9-8374-f69f1b01b4e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: {}\".format(device))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fe0VtVlAtl7s"
      },
      "source": [
        "## Fine-tune Teacher model on Dogs vs. Cats Prediction Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1686129170414
        },
        "id": "UyRflHdVtl7s"
      },
      "outputs": [],
      "source": [
        "teacher_model = Teacher().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(teacher_model.parameters(), lr=lr, amsgrad=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1686129264415
        },
        "id": "mGseX0Qotl7s",
        "outputId": "e75cde50-7cc0-4b91-9afb-d889347219b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 161/161 [01:51<00:00,  1.45 Iterations/s]\n",
            "Validating: 100%|██████████| 65/65 [00:45<00:00,  1.43Iterations/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 93.5%, Avg loss: 0.170367 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 161/161 [01:54<00:00,  1.41 Iterations/s]\n",
            "Validating: 100%|██████████| 65/65 [00:45<00:00,  1.43Iterations/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 94.7%, Avg loss: 0.135645 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 161/161 [01:55<00:00,  1.39 Iterations/s]\n",
            "Validating: 100%|██████████| 65/65 [00:45<00:00,  1.42Iterations/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 90.9%, Avg loss: 0.296362 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 161/161 [01:54<00:00,  1.41 Iterations/s]\n",
            "Validating: 100%|██████████| 65/65 [00:45<00:00,  1.41Iterations/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 95.6%, Avg loss: 0.108159 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 161/161 [01:56<00:00,  1.39 Iterations/s]\n",
            "Validating: 100%|██████████| 65/65 [00:46<00:00,  1.40Iterations/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 93.6%, Avg loss: 0.166912 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 161/161 [01:54<00:00,  1.40 Iterations/s]\n",
            "Validating: 100%|██████████| 65/65 [00:45<00:00,  1.41Iterations/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 94.7%, Avg loss: 0.137621 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 161/161 [01:56<00:00,  1.38 Iterations/s]\n",
            "Validating: 100%|██████████| 65/65 [00:45<00:00,  1.43Iterations/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 95.8%, Avg loss: 0.108259 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 161/161 [01:54<00:00,  1.40 Iterations/s]\n",
            "Validating: 100%|██████████| 65/65 [00:45<00:00,  1.43Iterations/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 96.1%, Avg loss: 0.100093 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 161/161 [01:54<00:00,  1.40 Iterations/s]\n",
            "Validating: 100%|██████████| 65/65 [00:45<00:00,  1.42Iterations/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 94.6%, Avg loss: 0.148131 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 161/161 [01:56<00:00,  1.38 Iterations/s]\n",
            "Validating: 100%|██████████| 65/65 [00:45<00:00,  1.41Iterations/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 94.8%, Avg loss: 0.133100 \n",
            "\n",
            "Done!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Fine-tune the final classification layers of the teacher model\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, teacher_model, loss_fn=criterion, optimizer=optimizer)\n",
        "    test(val_dataloader, teacher_model, loss_fn=criterion)\n",
        "print(\"Done!\")\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "k0H8Lox8tl7s"
      },
      "source": [
        "## Train Student Model without Knowledge Distillation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1686128872499
        },
        "id": "0CGrbV57tl7s",
        "outputId": "f3b63dff-598a-4dcd-f5ed-f2de48034043"
      },
      "outputs": [],
      "source": [
        "student_model = Student().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(student_model.parameters(), lr=lr, amsgrad=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1686128872513
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 161/161 [01:56<00:00,  1.38 Iterations/s]\n",
            "Validating: 100%|██████████| 65/65 [00:48<00:00,  1.34Iterations/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 64.7%, Avg loss: 0.634142 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 161/161 [01:54<00:00,  1.41 Iterations/s]\n",
            "Validating: 100%|██████████| 65/65 [00:45<00:00,  1.44Iterations/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 69.8%, Avg loss: 0.579216 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 161/161 [01:54<00:00,  1.41 Iterations/s]\n",
            "Validating: 100%|██████████| 65/65 [00:44<00:00,  1.45Iterations/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 67.7%, Avg loss: 0.621309 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 161/161 [01:53<00:00,  1.42 Iterations/s]\n",
            "Validating: 100%|██████████| 65/65 [00:45<00:00,  1.44Iterations/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 73.3%, Avg loss: 0.533942 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 161/161 [01:53<00:00,  1.41 Iterations/s]\n",
            "Validating: 100%|██████████| 65/65 [00:46<00:00,  1.41Iterations/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 74.1%, Avg loss: 0.520504 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 161/161 [01:55<00:00,  1.39 Iterations/s]\n",
            "Validating: 100%|██████████| 65/65 [00:46<00:00,  1.40Iterations/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 76.2%, Avg loss: 0.484073 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 161/161 [01:55<00:00,  1.40 Iterations/s]\n",
            "Validating: 100%|██████████| 65/65 [00:46<00:00,  1.40Iterations/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 77.1%, Avg loss: 0.490002 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 161/161 [01:56<00:00,  1.38 Iterations/s]\n",
            "Validating: 100%|██████████| 65/65 [00:45<00:00,  1.43Iterations/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 76.6%, Avg loss: 0.496116 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 161/161 [01:54<00:00,  1.40 Iterations/s]\n",
            "Validating: 100%|██████████| 65/65 [00:46<00:00,  1.41Iterations/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 79.5%, Avg loss: 0.445957 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 161/161 [01:55<00:00,  1.39 Iterations/s]\n",
            "Validating: 100%|██████████| 65/65 [00:46<00:00,  1.39Iterations/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 78.3%, Avg loss: 0.469271 \n",
            "\n",
            "Done!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, student_model, loss_fn=criterion, optimizer=optimizer)\n",
        "    test(val_dataloader, student_model, loss_fn=criterion)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "if6sVwg_tl7t"
      },
      "source": [
        "## Train Student Model with Knowledge Distillation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1686128872526
        },
        "id": "S8aMZvJltl7t"
      },
      "outputs": [],
      "source": [
        "student_model_distilled = Student()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(student_model_distilled.parameters(), lr=lr, amsgrad=True)\n",
        "teacher_model = teacher_model.to(device)\n",
        "student_model_distilled = student_model_distilled.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1686128872540
        },
        "id": "VyAFP1bhtl7t"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training with Distillation:   0%|          | 0/161 [00:00<?, ? Iterations/s]/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/torch/nn/functional.py:2904: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  warnings.warn(\n",
            "Training with Distillation: 100%|██████████| 161/161 [01:57<00:00,  1.37 Iterations/s]\n",
            "Validating: 100%|██████████| 65/65 [00:45<00:00,  1.42Iterations/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 63.0%, Avg loss: 0.646821 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training with Distillation: 100%|██████████| 161/161 [01:57<00:00,  1.37 Iterations/s]\n",
            "Validating: 100%|██████████| 65/65 [00:47<00:00,  1.38Iterations/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 65.8%, Avg loss: 0.617003 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training with Distillation: 100%|██████████| 161/161 [01:56<00:00,  1.38 Iterations/s]\n",
            "Validating: 100%|██████████| 65/65 [00:46<00:00,  1.39Iterations/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 68.8%, Avg loss: 0.593526 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training with Distillation: 100%|██████████| 161/161 [01:56<00:00,  1.38 Iterations/s]\n",
            "Validating: 100%|██████████| 65/65 [00:46<00:00,  1.41Iterations/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 73.8%, Avg loss: 0.519892 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training with Distillation: 100%|██████████| 161/161 [01:57<00:00,  1.37 Iterations/s]\n",
            "Validating: 100%|██████████| 65/65 [00:46<00:00,  1.41Iterations/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 76.2%, Avg loss: 0.495154 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training with Distillation: 100%|██████████| 161/161 [01:55<00:00,  1.39 Iterations/s]\n",
            "Validating: 100%|██████████| 65/65 [00:46<00:00,  1.41Iterations/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 75.8%, Avg loss: 0.495824 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training with Distillation: 100%|██████████| 161/161 [01:56<00:00,  1.38 Iterations/s]\n",
            "Validating: 100%|██████████| 65/65 [00:47<00:00,  1.38Iterations/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.484061 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training with Distillation: 100%|██████████| 161/161 [01:56<00:00,  1.38 Iterations/s]\n",
            "Validating: 100%|██████████| 65/65 [00:46<00:00,  1.40Iterations/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 78.9%, Avg loss: 0.460542 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training with Distillation: 100%|██████████| 161/161 [01:56<00:00,  1.38 Iterations/s]\n",
            "Validating: 100%|██████████| 65/65 [00:47<00:00,  1.38Iterations/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 77.9%, Avg loss: 0.480526 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training with Distillation: 100%|██████████| 161/161 [01:55<00:00,  1.39 Iterations/s]\n",
            "Validating: 100%|██████████| 65/65 [00:46<00:00,  1.39Iterations/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 80.2%, Avg loss: 0.433731 \n",
            "\n",
            "Done!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_with_distillation(train_dataloader, student_model_distilled, teacher_model, loss_fn=criterion, optimizer=optimizer)\n",
        "    test(val_dataloader, student_model_distilled, loss_fn=criterion)\n",
        "print(\"Done!\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1686128872564
        },
        "id": "1ZlGM5Fjtl7u",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputId": "532c4009-60a1-4dd1-adc4-675fe0ff6062"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating: 100%|██████████| 25/25 [00:20<00:00,  1.19Iterations/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 78.9%, Avg loss: 0.434410 \n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "test(test_dataloader, student_model_distilled, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1686128872576
        },
        "id": "h2gYI9fRtl7u",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputId": "38bd4f3a-45c8-4594-873d-e2294ce364e8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating: 100%|██████████| 25/25 [00:18<00:00,  1.33Iterations/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 76.0%, Avg loss: 0.492609 \n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "test(test_dataloader, student_model, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1686128872588
        },
        "id": "y28kROFRtl7u",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputId": "382d2606-b307-4725-a0e6-1a36a66a632c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating: 100%|██████████| 25/25 [00:18<00:00,  1.34Iterations/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 94.7%, Avg loss: 0.128114 \n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "test(test_dataloader, teacher_model, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model size teachermodel : 28.806MB\n",
            "model size student model: 1.321MB\n"
          ]
        }
      ],
      "source": [
        "# Calculate the model size \n",
        "model_size_student = get_model_size(student_model)\n",
        "model_size_teacher = get_model_size(teacher_model)\n",
        "\n",
        "print('model size teachermodel : {:.3f}MB'.format(model_size_teacher))\n",
        "print('model size student model: {:.3f}MB'.format(model_size_student))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "#torch.save(teacher_model, '../models/teacher_model_densenet121_10epochs.pt')\n",
        "#torch.save(student_model, '../models/student_model_10epochs.pt')\n",
        "#torch.save(student_model_distilled, '../models/student_model_distilled_10epochs.pt')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "g0kmZs-qtl7u",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# References \n",
        "\n",
        "\n",
        "\n",
        "## Dataroots blog posts on model compression\n",
        "- https://dataroots.io/research/contributions/deep-learning-model-compression/?ref=dataroots.ghost.io\n",
        "- https://dataroots.io/research/contributions/model_compression/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - Pytorch and Tensorflow",
      "language": "python",
      "name": "python38-azureml-pt-tf"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
