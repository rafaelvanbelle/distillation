{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/pdf/1503.02531.pdf"
      ],
      "metadata": {
        "id": "liHnKN3Dtl7e",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# Knowledge Distillation "
      ],
      "metadata": {
        "id": "AtsMO6_Jtl7g"
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "- Current SOTA performance in AI and ML is mainly driven by large and complex deep neural network models that consist of billions of model parameters. \n",
        "\n",
        "- deploying large complex models on constrained devices (e.g. edge devices) is not straightforward\n",
        "\n",
        "- while deep learning models often achieve excellent accuracy, they often fail to meet other requirements such as  latency and memory footprint. \n",
        "\n",
        "- Knowledge distillation distills the knowlegde of a larger, complex model into a smaller and easier to deploy model. \n",
        "\n",
        "- The complex model is called the 'teacher' and the smaller model is referred to as the 'student'. \n",
        "\n",
        "### different kinds of knowledge\n",
        "- response based knowledge\n",
        "- Feature based knowledge\n",
        "- Relation-based knowledge\n",
        "\n",
        "### Different kinds of training\n",
        "- Offline distillation\n",
        "- Online distillation\n",
        "- Self distillation\n",
        "\n",
        "### Real world examples\n",
        "- DistilBERT\n",
        "-> add short summary (e.g. smaller by 40%, whilst retaining xx % of performance). \n"
      ],
      "metadata": {
        "id": "_2-AfH_Gtl7h"
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "[TODO] write introduction for knowledge distillation\n",
        "[TODO] Add relevant references at the end of the notebook"
      ],
      "metadata": {
        "id": "lk40LKMitl7h"
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## What is Knowledge Distillation exactly?"
      ],
      "metadata": {
        "id": "8IoxDKN7tl7i"
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## In this tutorial\n",
        "\n",
        "- [Setup](#Setup)\n",
        "- [Functions](#Functions)\n",
        "- [Data](#load-data)\n",
        "- [Experiments](#experiments)"
      ],
      "metadata": {
        "id": "sQ0snsRmtl7i"
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "MSH8xrqitl7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "import numpy as np\n",
        "import PIL\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms"
      ],
      "outputs": [],
      "execution_count": 40,
      "metadata": {
        "gather": {
          "logged": 1686129162790
        },
        "id": "P5Ta6AFetl7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_val_test_split = (0.7,0.2,0.1)\n",
        "batch_size = 100\n",
        "num_workers=6\n",
        "\n",
        "data_dir = '../data/train'\n",
        "epochs = 10\n",
        "lr = 0.001"
      ],
      "outputs": [],
      "execution_count": 41,
      "metadata": {
        "gather": {
          "logged": 1686129163516
        },
        "id": "5CuFi559tl7k"
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jT_vp_f8tl7l"
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "yLf0K0Gftl7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_size(model):\n",
        "    \"\"\"function to calculate the model size in MB\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): pytorch model\n",
        "    \"\"\"\n",
        "    param_size = 0\n",
        "    for param in model.parameters():\n",
        "        param_size += param.nelement() * param.element_size()\n",
        "    \n",
        "    buffer_size = 0\n",
        "    for buffer in model.buffers():\n",
        "        buffer_size += buffer.nelement() * buffer.element_size()\n",
        "    \n",
        "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "\n",
        "    return size_all_mb"
      ],
      "outputs": [],
      "execution_count": 42,
      "metadata": {
        "id": "cizHMWROtl7l",
        "gather": {
          "logged": 1686129163776
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DistillationLoss:\n",
        "\n",
        "    \"\"\"Custom loss calculcation combining\n",
        "    the loss of the student model with the distillation loss\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, student_loss, temperature=1, alpha=0.25):\n",
        "        self.student_loss = student_loss\n",
        "        self.distillation_loss = nn.KLDivLoss()\n",
        "        self.temperature = 1\n",
        "        self.alpha = 0.25\n",
        "\n",
        "    def __call__(self, student_logits, student_target_loss, teacher_logits):\n",
        "        distillation_loss = self.distillation_loss(\n",
        "            F.log_softmax(student_logits / self.temperature, dim=1),\n",
        "            F.softmax(teacher_logits/self.temperature, dim=1))\n",
        "        loss = (1 - self.alpha) * student_target_loss \\\n",
        "            + self.alpha * distillation_loss\n",
        "        return loss\n"
      ],
      "outputs": [],
      "execution_count": 43,
      "metadata": {
        "id": "bf9O-R7Ttl7m",
        "gather": {
          "logged": 1686129164457
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for X, y in tqdm.tqdm(dataloader, desc = \"Training\", unit = \" Iterations\"):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #if batch % 100 == 0:\n",
        "        #    loss, current = loss.item(), (batch + 1) * len(X)\n",
        "        #    print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in tqdm.tqdm(dataloader, desc = \"Validating\", unit=\"Iterations\"):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "def train_with_distillation(dataloader, student_model, teacher_model, loss_fn, optimizer):\n",
        "    distillation_loss = DistillationLoss(student_loss=loss_fn)\n",
        "    size = len(dataloader.dataset)\n",
        "    student_model.train()\n",
        "    teacher_model.eval()\n",
        "\n",
        "    for X, y in tqdm.tqdm(dataloader, desc = \"Training with Distillation\", unit = \" Iterations\"):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred_student = student_model(X)\n",
        "        pred_teacher = teacher_model(X)\n",
        "\n",
        "        student_target_loss = loss_fn(pred_student, y)\n",
        "        loss = distillation_loss(pred_student, student_target_loss, pred_teacher)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        "
      ],
      "outputs": [],
      "execution_count": 44,
      "metadata": {
        "id": "gTe4-yoLtl7m",
        "gather": {
          "logged": 1686129164584
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Student/Teacher Models"
      ],
      "metadata": {
        "id": "Yro6rNOhtl7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Teacher(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = torchvision.models.densenet121(weights='DEFAULT')\n",
        "        for params in self.model.parameters():\n",
        "            params.requires_grad_ = False\n",
        "\n",
        "        num_ftrs = self.model.classifier.in_features\n",
        "        self.model.classifier = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 500),\n",
        "            nn.Linear(500, 2)\n",
        "            )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x"
      ],
      "outputs": [],
      "execution_count": 45,
      "metadata": {
        "gather": {
          "logged": 1686129164726
        },
        "id": "hcVuMmtLtl7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Student(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # onvolutional layers (3,16,32)\n",
        "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size=(5, 5), stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size=(5, 5), stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size=(3, 3), padding=1)\n",
        "\n",
        "        # conected layers\n",
        "        self.fc1 = nn.Linear(in_features= 64 * 3 * 3, out_features=500)\n",
        "        self.fc2 = nn.Linear(in_features=500, out_features=50)\n",
        "        self.fc3 = nn.Linear(in_features=50, out_features=2)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ],
      "outputs": [],
      "execution_count": 46,
      "metadata": {
        "gather": {
          "logged": 1686129165406
        },
        "id": "NUtAflDvtl7n"
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "3pzxfgHdtl7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "#files.upload()"
      ],
      "outputs": [],
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "a2GU5rdsuKgS",
        "outputId": "89cbbb5b-abff-45db-ff80-3f3c816af71e",
        "gather": {
          "logged": 1686129165676
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!rm -r ~/.kaggle\n",
        "#!mkdir ~/.kaggle\n",
        "#!mv ./kaggle.json ~/.kaggle/\n",
        "#!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "outputs": [],
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvfLGKfPubZ1",
        "outputId": "7abef924-dc94-473a-a6fa-3909d23e90ba",
        "gather": {
          "logged": 1686129166374
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!kaggle competitions download -c dogs-vs-cats"
      ],
      "outputs": [],
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efZZxlaPuiBk",
        "outputId": "65f9dd63-cb65-46f7-a3bd-43e4c8624571",
        "gather": {
          "logged": 1686129166605
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip -o -q dogs-vs-cats.zip -d ./data/ \n",
        "#!unzip -o -q ./data/train.zip -d ./data/ "
      ],
      "outputs": [],
      "execution_count": 50,
      "metadata": {
        "id": "NGEtHupAu35h",
        "gather": {
          "logged": 1686129166758
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "files = os.listdir(data_dir)\n",
        "files = [f for f in files if '.jpg' in f]\n",
        "#files = random.sample(files, 10000)"
      ],
      "outputs": [],
      "execution_count": 51,
      "metadata": {
        "id": "Awx8hMAbxnwR",
        "gather": {
          "logged": 1686129168356
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.ColorJitter(),\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.Resize((128,128)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((128,128)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "outputs": [],
      "execution_count": 52,
      "metadata": {
        "gather": {
          "logged": 1686129168482
        },
        "id": "Ngk9tJKgtl7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DogsVsCatsDataset(Dataset):\n",
        "    def __init__(self, file_list, dir, mode='train', transform = val_transform):\n",
        "        self.file_list = file_list\n",
        "        self.dir = dir\n",
        "        #self.mode= mode\n",
        "        self.transform = transform\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img = PIL.Image.open(os.path.join(self.dir, self.file_list[idx]))\n",
        "        img = self.transform(img)\n",
        "        img = np.array(img)\n",
        "        if 'dog' in self.file_list[idx]:\n",
        "            self.label = 1\n",
        "        else:\n",
        "            self.label = 0\n",
        "        return img.astype('float32'), self.label\n",
        "\n",
        "\n",
        "train_files, test_files = train_test_split(files, \n",
        "                                    test_size=train_val_test_split[2], \n",
        "                                    random_state=42\n",
        "                                    )\n",
        "train_files, val_files = train_test_split(train_files,\n",
        "                                    test_size=train_val_test_split[1]/train_val_test_split[0], \n",
        "                                    random_state=42\n",
        "                                    )\n",
        "\n",
        "train_dataset = DogsVsCatsDataset(train_files, dir = data_dir, transform = train_transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "val_dataset = DogsVsCatsDataset(val_files, dir = data_dir, transform = val_transform)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size = batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "test_dataset = DogsVsCatsDataset(test_files, dir = data_dir, transform = val_transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle=False, num_workers=num_workers)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 6, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n"
        }
      ],
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32kWesurxanW",
        "outputId": "3a4becac-fed9-484b-e9fc-73f73fbaf6af",
        "gather": {
          "logged": 1686129168589
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# Experiments"
      ],
      "metadata": {
        "id": "r-R3csrdtl7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: {}\".format(device))\n",
        "\n",
        "student_model = Student().to(device)\n",
        "teacher_model = Teacher().to(device)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Device: cuda\n"
        }
      ],
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1686129168859
        },
        "id": "uGcjkBwFtl7q",
        "outputId": "ec052570-5daa-4ef9-8374-f69f1b01b4e1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_size_student = get_model_size(student_model)\n",
        "model_size_teacher = get_model_size(teacher_model)\n",
        "\n",
        "print('model size teachermodel : {:.3f}MB'.format(model_size_teacher))\n",
        "print('model size student model: {:.3f}MB'.format(model_size_student))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "model size teachermodel : 28.806MB\nmodel size student model: 1.321MB\n"
        }
      ],
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMfmh1lLtl7r",
        "outputId": "32e1836a-3d0b-4bb0-d346-72d76b38202c",
        "gather": {
          "logged": 1686129169413
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Teacher model"
      ],
      "metadata": {
        "id": "fe0VtVlAtl7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(teacher_model.parameters(), lr=lr, amsgrad=True)\n"
      ],
      "outputs": [],
      "execution_count": 56,
      "metadata": {
        "gather": {
          "logged": 1686129170414
        },
        "id": "UyRflHdVtl7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune the final classification layers of the teacher model\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, teacher_model, loss_fn=criterion, optimizer=optimizer)\n",
        "    test(val_dataloader, teacher_model, loss_fn=criterion)\n",
        "print(\"Done!\")\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1\n-------------------------------\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Training:  82%|████████▏ | 132/161 [01:28<00:19,  1.49 Iterations/s]\n"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[57], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     test(val_dataloader, teacher_model, loss_fn\u001b[38;5;241m=\u001b[39mcriterion)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[44], line 4\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      2\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(dataloader, desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Iterations\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      5\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Compute prediction error\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/torch/utils/data/dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 652\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    656\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1330\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1330\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1333\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1296\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1295\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1296\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1297\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1298\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1134\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/queues.py:107\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    106\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGseX0Qotl7s",
        "outputId": "e75cde50-7cc0-4b91-9afb-d889347219b5",
        "gather": {
          "logged": 1686129264415
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Train Student Model from Scratch"
      ],
      "metadata": {
        "id": "k0H8Lox8tl7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(student_model.parameters(), lr=lr, amsgrad=True)\n",
        "student_model = student_model.to(device)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686128872499
        },
        "id": "0CGrbV57tl7s",
        "outputId": "f3b63dff-598a-4dcd-f5ed-f2de48034043"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, student_model, loss_fn=criterion, optimizer=optimizer)\n",
        "    test(val_dataloader, student_model, loss_fn=criterion)\n",
        "print(\"Done!\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1686128872513
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Train Student Model with Knowledge Distillation"
      ],
      "metadata": {
        "id": "if6sVwg_tl7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "student_model_distilled = Student()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(student_model_distilled.parameters(), lr=lr, amsgrad=True)\n",
        "teacher_model = teacher_model.to(device)\n",
        "student_model_distilled = student_model_distilled.to(device)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686128872526
        },
        "id": "S8aMZvJltl7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_with_distillation(train_dataloader, student_model_distilled, teacher_model, loss_fn=criterion, optimizer=optimizer)\n",
        "    test(val_dataloader, student_model_distilled, loss_fn=criterion)\n",
        "print(\"Done!\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686128872540
        },
        "id": "VyAFP1bhtl7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_with_distillation(student_model=student_model_distilled, teacher_model=teacher_model)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686128872552
        },
        "id": "IbpHCssAtl7u",
        "outputId": "b9d99d3a-3373-440a-9cd7-aa839d700af5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test(test_dataloader, student_model_distilled, criterion)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686128872564
        },
        "id": "1ZlGM5Fjtl7u",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputId": "532c4009-60a1-4dd1-adc4-675fe0ff6062"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test(test_dataloader, student_model, criterion)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686128872576
        },
        "id": "h2gYI9fRtl7u",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputId": "38bd4f3a-45c8-4594-873d-e2294ce364e8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test(test_dataloader, teacher_model, criterion)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1686128872588
        },
        "id": "y28kROFRtl7u",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputId": "382d2606-b307-4725-a0e6-1a36a66a632c"
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# References "
      ],
      "metadata": {
        "id": "g0kmZs-qtl7u",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "orig_nbformat": 4,
    "accelerator": "GPU",
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}