{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "liHnKN3Dtl7e",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "https://arxiv.org/pdf/1503.02531.pdf"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AtsMO6_Jtl7g"
      },
      "source": [
        "# Knowledge Distillation "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_2-AfH_Gtl7h"
      },
      "source": [
        "- Current SOTA performance in AI and ML is mainly driven by large and complex deep neural network models that consist of billions of model parameters. \n",
        "\n",
        "- deploying large complex models on constrained devices (e.g. edge devices) is not straightforward\n",
        "\n",
        "- while deep learning models often achieve excellent accuracy, they often fail to meet other requirements such as  latency and memory footprint. \n",
        "\n",
        "- Knowledge distillation distills the knowlegde of a larger, complex model into a smaller and easier to deploy model. \n",
        "\n",
        "- The complex model is called the 'teacher' and the smaller model is referred to as the 'student'. \n",
        "\n",
        "### different kinds of knowledge\n",
        "- response based knowledge\n",
        "- Feature based knowledge\n",
        "- Relation-based knowledge\n",
        "\n",
        "### Different kinds of training\n",
        "- Offline distillation\n",
        "- Online distillation\n",
        "- Self distillation\n",
        "\n",
        "### Real world examples\n",
        "- DistilBERT\n",
        "-> add short summary (e.g. smaller by 40%, whilst retaining xx % of performance). \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lk40LKMitl7h"
      },
      "source": [
        "[TODO] write introduction for knowledge distillation\n",
        "[TODO] Add relevant references at the end of the notebook"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8IoxDKN7tl7i"
      },
      "source": [
        "## What is Knowledge Distillation exactly?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ0snsRmtl7i"
      },
      "source": [
        "## In this tutorial\n",
        "\n",
        "- [Setup](#Setup)\n",
        "- [Functions](#Functions)\n",
        "- [Data](#load-data)\n",
        "- [Experiments](#experiments)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MSH8xrqitl7i"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1686129162790
        },
        "id": "P5Ta6AFetl7k"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import tqdm\n",
        "import numpy as np\n",
        "import PIL\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1686129163516
        },
        "id": "5CuFi559tl7k"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_val_test_split = (0.7,0.2,0.1)\n",
        "batch_size = 100\n",
        "num_workers=6\n",
        "\n",
        "data_dir = '../data/train'\n",
        "epochs = 10\n",
        "lr = 0.001"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jT_vp_f8tl7l"
      },
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yLf0K0Gftl7l"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1686129163776
        },
        "id": "cizHMWROtl7l"
      },
      "outputs": [],
      "source": [
        "def get_model_size(model):\n",
        "    \"\"\"function to calculate the model size in MB\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): pytorch model\n",
        "    \"\"\"\n",
        "    param_size = 0\n",
        "    for param in model.parameters():\n",
        "        param_size += param.nelement() * param.element_size()\n",
        "    \n",
        "    buffer_size = 0\n",
        "    for buffer in model.buffers():\n",
        "        buffer_size += buffer.nelement() * buffer.element_size()\n",
        "    \n",
        "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "\n",
        "    return size_all_mb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1686129164457
        },
        "id": "bf9O-R7Ttl7m"
      },
      "outputs": [],
      "source": [
        "class DistillationLoss:\n",
        "\n",
        "    \"\"\"Custom loss calculcation combining\n",
        "    the loss of the student model with the distillation loss\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, student_loss, temperature=1, alpha=0.25):\n",
        "        self.student_loss = student_loss\n",
        "        self.distillation_loss = nn.KLDivLoss()\n",
        "        self.temperature = 1\n",
        "        self.alpha = 0.25\n",
        "\n",
        "    def __call__(self, student_logits, student_target_loss, teacher_logits):\n",
        "        distillation_loss = self.distillation_loss(\n",
        "            F.log_softmax(student_logits / self.temperature, dim=1),\n",
        "            F.softmax(teacher_logits/self.temperature, dim=1))\n",
        "        loss = (1 - self.alpha) * student_target_loss \\\n",
        "            + self.alpha * distillation_loss\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1686129164584
        },
        "id": "gTe4-yoLtl7m"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for X, y in tqdm.tqdm(dataloader, desc = \"Training\", unit = \" Iterations\"):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #if batch % 100 == 0:\n",
        "        #    loss, current = loss.item(), (batch + 1) * len(X)\n",
        "        #    print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in tqdm.tqdm(dataloader, desc = \"Validating\", unit=\"Iterations\"):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "def train_with_distillation(dataloader, student_model, teacher_model, loss_fn, optimizer):\n",
        "    distillation_loss = DistillationLoss(student_loss=loss_fn)\n",
        "    size = len(dataloader.dataset)\n",
        "    student_model.train()\n",
        "    teacher_model.eval()\n",
        "\n",
        "    for X, y in tqdm.tqdm(dataloader, desc = \"Training with Distillation\", unit = \" Iterations\"):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred_student = student_model(X)\n",
        "        pred_teacher = teacher_model(X)\n",
        "\n",
        "        student_target_loss = loss_fn(pred_student, y)\n",
        "        loss = distillation_loss(pred_student, student_target_loss, pred_teacher)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Yro6rNOhtl7n"
      },
      "source": [
        "## Student/Teacher Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1686129164726
        },
        "id": "hcVuMmtLtl7n"
      },
      "outputs": [],
      "source": [
        "class Teacher(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = torchvision.models.densenet121(weights='DEFAULT')\n",
        "        for params in self.model.parameters():\n",
        "            params.requires_grad_ = False\n",
        "\n",
        "        num_ftrs = self.model.classifier.in_features\n",
        "        self.model.classifier = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 500),\n",
        "            nn.Linear(500, 2)\n",
        "            )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1686129165406
        },
        "id": "NUtAflDvtl7n"
      },
      "outputs": [],
      "source": [
        "class Student(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # onvolutional layers (3,16,32)\n",
        "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size=(5, 5), stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size=(5, 5), stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size=(3, 3), padding=1)\n",
        "\n",
        "        # conected layers\n",
        "        self.fc1 = nn.Linear(in_features= 64 * 3 * 3, out_features=500)\n",
        "        self.fc2 = nn.Linear(in_features=500, out_features=50)\n",
        "        self.fc3 = nn.Linear(in_features=50, out_features=2)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3pzxfgHdtl7o"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "gather": {
          "logged": 1686129165676
        },
        "id": "a2GU5rdsuKgS",
        "outputId": "89cbbb5b-abff-45db-ff80-3f3c816af71e"
      },
      "outputs": [],
      "source": [
        "#from google.colab import files\n",
        "#files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1686129166374
        },
        "id": "OvfLGKfPubZ1",
        "outputId": "7abef924-dc94-473a-a6fa-3909d23e90ba"
      },
      "outputs": [],
      "source": [
        "#!rm -r ~/.kaggle\n",
        "#!mkdir ~/.kaggle\n",
        "#!mv ./kaggle.json ~/.kaggle/\n",
        "#!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1686129166605
        },
        "id": "efZZxlaPuiBk",
        "outputId": "65f9dd63-cb65-46f7-a3bd-43e4c8624571"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: /anaconda/envs/azureml_py38/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/azureuser/.kaggle/kaggle.json'\n",
            "401 - Unauthorized\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1686129166758
        },
        "id": "NGEtHupAu35h"
      },
      "outputs": [],
      "source": [
        "#!unzip -o -q dogs-vs-cats.zip -d ./data/ \n",
        "#!unzip -o -q ./data/train.zip -d ./data/ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1686129168356
        },
        "id": "Awx8hMAbxnwR"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "files = os.listdir(data_dir)\n",
        "files = [f for f in files if '.jpg' in f]\n",
        "#files = random.sample(files, 10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1686129168482
        },
        "id": "Ngk9tJKgtl7p"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.ColorJitter(),\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.Resize((128,128)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((128,128)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1686129168589
        },
        "id": "32kWesurxanW",
        "outputId": "3a4becac-fed9-484b-e9fc-73f73fbaf6af"
      },
      "outputs": [],
      "source": [
        "class DogsVsCatsDataset(Dataset):\n",
        "    def __init__(self, file_list, dir, mode='train', transform = val_transform):\n",
        "        self.file_list = file_list\n",
        "        self.dir = dir\n",
        "        #self.mode= mode\n",
        "        self.transform = transform\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img = PIL.Image.open(os.path.join(self.dir, self.file_list[idx]))\n",
        "        img = self.transform(img)\n",
        "        img = np.array(img)\n",
        "        if 'dog' in self.file_list[idx]:\n",
        "            self.label = 1\n",
        "        else:\n",
        "            self.label = 0\n",
        "        return img.astype('float32'), self.label\n",
        "\n",
        "\n",
        "train_files, test_files = train_test_split(files, \n",
        "                                    test_size=train_val_test_split[2], \n",
        "                                    random_state=42\n",
        "                                    )\n",
        "train_files, val_files = train_test_split(train_files,\n",
        "                                    test_size=train_val_test_split[1]/train_val_test_split[0], \n",
        "                                    random_state=42\n",
        "                                    )\n",
        "\n",
        "train_dataset = DogsVsCatsDataset(train_files, dir = data_dir, transform = train_transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "val_dataset = DogsVsCatsDataset(val_files, dir = data_dir, transform = val_transform)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size = batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "test_dataset = DogsVsCatsDataset(test_files, dir = data_dir, transform = val_transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle=False, num_workers=num_workers)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "r-R3csrdtl7q"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1686129168859
        },
        "id": "uGcjkBwFtl7q",
        "outputId": "ec052570-5daa-4ef9-8374-f69f1b01b4e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: {}\".format(device))\n",
        "\n",
        "student_model = Student().to(device)\n",
        "teacher_model = Teacher().to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1686129169413
        },
        "id": "hMfmh1lLtl7r",
        "outputId": "32e1836a-3d0b-4bb0-d346-72d76b38202c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model size teachermodel : 28.806MB\n",
            "model size student model: 1.321MB\n"
          ]
        }
      ],
      "source": [
        "model_size_student = get_model_size(student_model)\n",
        "model_size_teacher = get_model_size(teacher_model)\n",
        "\n",
        "print('model size teachermodel : {:.3f}MB'.format(model_size_teacher))\n",
        "print('model size student model: {:.3f}MB'.format(model_size_student))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fe0VtVlAtl7s"
      },
      "source": [
        "## Teacher model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1686129170414
        },
        "id": "UyRflHdVtl7s"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(teacher_model.parameters(), lr=lr, amsgrad=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1686129264415
        },
        "id": "mGseX0Qotl7s",
        "outputId": "e75cde50-7cc0-4b91-9afb-d889347219b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|          | 0/161 [00:00<?, ? Iterations/s]"
          ]
        }
      ],
      "source": [
        "# Fine-tune the final classification layers of the teacher model\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, teacher_model, loss_fn=criterion, optimizer=optimizer)\n",
        "    test(val_dataloader, teacher_model, loss_fn=criterion)\n",
        "print(\"Done!\")\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "k0H8Lox8tl7s"
      },
      "source": [
        "## Train Student Model from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686128872499
        },
        "id": "0CGrbV57tl7s",
        "outputId": "f3b63dff-598a-4dcd-f5ed-f2de48034043"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(student_model.parameters(), lr=lr, amsgrad=True)\n",
        "student_model = student_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686128872513
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, student_model, loss_fn=criterion, optimizer=optimizer)\n",
        "    test(val_dataloader, student_model, loss_fn=criterion)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "if6sVwg_tl7t"
      },
      "source": [
        "## Train Student Model with Knowledge Distillation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686128872526
        },
        "id": "S8aMZvJltl7t"
      },
      "outputs": [],
      "source": [
        "student_model_distilled = Student()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(student_model_distilled.parameters(), lr=lr, amsgrad=True)\n",
        "teacher_model = teacher_model.to(device)\n",
        "student_model_distilled = student_model_distilled.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686128872540
        },
        "id": "VyAFP1bhtl7t"
      },
      "outputs": [],
      "source": [
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_with_distillation(train_dataloader, student_model_distilled, teacher_model, loss_fn=criterion, optimizer=optimizer)\n",
        "    test(val_dataloader, student_model_distilled, loss_fn=criterion)\n",
        "print(\"Done!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686128872552
        },
        "id": "IbpHCssAtl7u",
        "outputId": "b9d99d3a-3373-440a-9cd7-aa839d700af5"
      },
      "outputs": [],
      "source": [
        "train_with_distillation(student_model=student_model_distilled, teacher_model=teacher_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686128872564
        },
        "id": "1ZlGM5Fjtl7u",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputId": "532c4009-60a1-4dd1-adc4-675fe0ff6062"
      },
      "outputs": [],
      "source": [
        "test(test_dataloader, student_model_distilled, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686128872576
        },
        "id": "h2gYI9fRtl7u",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputId": "38bd4f3a-45c8-4594-873d-e2294ce364e8"
      },
      "outputs": [],
      "source": [
        "test(test_dataloader, student_model, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1686128872588
        },
        "id": "y28kROFRtl7u",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputId": "382d2606-b307-4725-a0e6-1a36a66a632c"
      },
      "outputs": [],
      "source": [
        "test(test_dataloader, teacher_model, criterion)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "g0kmZs-qtl7u",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# References "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "kernelspec": {
      "display_name": "azureml_py38_PT_TF",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
